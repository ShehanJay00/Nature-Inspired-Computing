import numpy as np
import matplotlib.pyplot as plt

# Parameters
NUM_AGENTS = 50
ITERATIONS = 200
BOUNDARY = 100
GOAL = np.array([90, 90])  # Goal position
OBSTACLE_COUNT = 5

# PSO parameters
W = 0.5         # Inertia weight
C1 = 1.5        # Cognitive component
C2 = 1.5        # Social component
VELOCITY_LIMIT = 3

# Agent behavior weights
COHESION_WEIGHT = 1.0
SEPARATION_WEIGHT = 2.0
ALIGNMENT_WEIGHT = 1.0
GOAL_ATTRACTION_WEIGHT = 2.5
AVOID_OBSTACLE_WEIGHT = 3.0

# Initialize agents
agents = np.random.rand(NUM_AGENTS, 2) * BOUNDARY  # Agent positions
velocities = np.random.rand(NUM_AGENTS, 2) * VELOCITY_LIMIT  # Agent velocities
personal_best_positions = np.copy(agents)
global_best_position = GOAL

# Generate obstacles
obstacles = np.random.rand(OBSTACLE_COUNT, 2) * BOUNDARY
OBSTACLE_RADIUS = 6

# Helper functions
def cohesion(agents):
    center_of_mass = np.mean(agents, axis=0)
    return center_of_mass - agents

def separation(agents):
    force = np.zeros_like(agents)
    for i, agent in enumerate(agents):
        for j, neighbor in enumerate(agents):
            if i != j and np.linalg.norm(agent - neighbor) < 5:
                force[i] -= (neighbor - agent)
    return force

def alignment(agents, velocities):
    avg_velocity = np.mean(velocities, axis=0)
    return avg_velocity - velocities

def avoid_obstacles(agents, obstacles):
    avoidance = np.zeros_like(agents)
    for i, agent in enumerate(agents):
        for obs in obstacles:
            dist = np.linalg.norm(agent - obs)
            if dist < OBSTACLE_RADIUS:
                avoidance[i] += (agent - obs) / dist**2
    return avoidance

def attraction_to_goal(agents, goal):
    return goal - agents

def update_velocity(vel, pos, personal_best, global_best):
    inertia = W * vel
    cognitive = C1 * np.random.rand(*vel.shape) * (personal_best - pos)
    social = C2 * np.random.rand(*vel.shape) * (global_best - pos)
    new_velocity = inertia + cognitive + social
    return np.clip(new_velocity, -VELOCITY_LIMIT, VELOCITY_LIMIT)

def update_positions(pos, vel):
    new_positions = pos + vel
    return np.clip(new_positions, 0, BOUNDARY)

# Visualization function
def plot_simulation(agents, velocities, obstacles, goal, iteration):
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.set_xlim(0, BOUNDARY)
    ax.set_ylim(0, BOUNDARY)
    ax.set_title(f"Crowd Simulation - Iteration {iteration}")
    
    # Add ground
    ground = np.ones((BOUNDARY, BOUNDARY, 3)) * 0.9  # Light grey ground
    ax.imshow(ground, extent=[0, BOUNDARY, 0, BOUNDARY], origin='lower')
    
    # Plot arrows for agents
    ax.quiver(
        agents[:, 0], agents[:, 1], velocities[:, 0], velocities[:, 1],
        angles='xy', scale_units='xy', scale=0.7, color='blue', label='Agents'
    )
    
    # Plot obstacles
    for obs in obstacles:
        circle = plt.Circle(obs, OBSTACLE_RADIUS, color='red', alpha=0.5)
        ax.add_artist(circle)
    
    # Plot goal
    ax.scatter(goal[0], goal[1], c='green', s=300, marker='X', label='Goal')
    
    plt.legend(loc='upper left')
    plt.show()

# PSO loop
for iteration in range(ITERATIONS):
    cohesion_force = COHESION_WEIGHT * cohesion(agents)
    separation_force = SEPARATION_WEIGHT * separation(agents)
    alignment_force = ALIGNMENT_WEIGHT * alignment(agents, velocities)
    goal_force = GOAL_ATTRACTION_WEIGHT * attraction_to_goal(agents, GOAL)
    obstacle_force = AVOID_OBSTACLE_WEIGHT * avoid_obstacles(agents, obstacles)
    
    velocities += cohesion_force + separation_force + alignment_force + goal_force + obstacle_force
    velocities = np.clip(velocities, -VELOCITY_LIMIT, VELOCITY_LIMIT)
    
    agents = update_positions(agents, velocities)
    
    for i, agent in enumerate(agents):
        if np.linalg.norm(agent - GOAL) < np.linalg.norm(personal_best_positions[i] - GOAL):
            personal_best_positions[i] = agent
    
    if iteration % 10 == 0:
        plot_simulation(agents, velocities, obstacles, GOAL, iteration)






import numpy as np
import matplotlib.pyplot as plt
from matplotlib.offsetbox import OffsetImage, AnnotationBbox

# Parameters
NUM_AGENTS = 50
ITERATIONS = 200
BOUNDARY = 100
GOAL = np.array([90, 90])  # Goal position
OBSTACLE_COUNT = 5

# PSO parameters
W = 0.5         # Inertia weight
C1 = 1.5        # Cognitive component
C2 = 1.5        # Social component
VELOCITY_LIMIT = 3

# Agent behavior weights
COHESION_WEIGHT = 1.0
SEPARATION_WEIGHT = 2.0
ALIGNMENT_WEIGHT = 1.0
GOAL_ATTRACTION_WEIGHT = 2.5
AVOID_OBSTACLE_WEIGHT = 3.0

# Initialize agents
agents = np.random.rand(NUM_AGENTS, 2) * BOUNDARY  # Agent positions
velocities = np.random.rand(NUM_AGENTS, 2) * VELOCITY_LIMIT  # Agent velocities
personal_best_positions = np.copy(agents)
global_best_position = GOAL

# Generate obstacles
obstacles = np.random.rand(OBSTACLE_COUNT, 2) * BOUNDARY
OBSTACLE_RADIUS = 7

# Helper functions
def cohesion(agents):
    center_of_mass = np.mean(agents, axis=0)
    return center_of_mass - agents

def separation(agents):
    force = np.zeros_like(agents)
    for i, agent in enumerate(agents):
        for j, neighbor in enumerate(agents):
            if i != j and np.linalg.norm(agent - neighbor) < 5:
                force[i] -= (neighbor - agent)
    return force

def alignment(agents, velocities):
    avg_velocity = np.mean(velocities, axis=0)
    return avg_velocity - velocities

def avoid_obstacles(agents, obstacles):
    avoidance = np.zeros_like(agents)
    for i, agent in enumerate(agents):
        for obs in obstacles:
            dist = np.linalg.norm(agent - obs)
            if dist < OBSTACLE_RADIUS:
                avoidance[i] += (agent - obs) / dist**2
    return avoidance

def attraction_to_goal(agents, goal):
    return goal - agents

def update_velocity(vel, pos, personal_best, global_best):
    inertia = W * vel
    cognitive = C1 * np.random.rand(*vel.shape) * (personal_best - pos)
    social = C2 * np.random.rand(*vel.shape) * (global_best - pos)
    new_velocity = inertia + cognitive + social
    return np.clip(new_velocity, -VELOCITY_LIMIT, VELOCITY_LIMIT)

def update_positions(pos, vel):
    new_positions = pos + vel
    return np.clip(new_positions, 0, BOUNDARY)

def add_human_icon(ax, positions, img_path, scale=0.5):
    img = plt.imread(img_path)
    for pos in positions:
        image = OffsetImage(img, zoom=scale)
        ab = AnnotationBbox(image, pos, frameon=False)
        ax.add_artist(ab)

# Visualization function
def plot_simulation(agents, obstacles, goal, iteration):
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.set_xlim(0, BOUNDARY)
    ax.set_ylim(0, BOUNDARY)
    ax.set_title(f"Crowd Simulation - Iteration {iteration}")
    
    # Add ground
    ground = np.ones((BOUNDARY, BOUNDARY, 3)) * 0.8  # Light grey ground
    ax.imshow(ground, extent=[0, BOUNDARY, 0, BOUNDARY])
    
    # Plot obstacles
    for obs in obstacles:
        circle = plt.Circle(obs, OBSTACLE_RADIUS, color='red', alpha=0.5)
        ax.add_artist(circle)
    
    # Plot goal
    ax.scatter(goal[0], goal[1], c='green', s=200, marker='X', label='Goal')
    
    # Add human icons for agents
    human_icon_path = 'human_icon.png'  # Replace with path to human icon
    add_human_icon(ax, agents, human_icon_path, scale=0.1)
    
    plt.legend()
    plt.show()

# PSO loop
for iteration in range(ITERATIONS):
    cohesion_force = COHESION_WEIGHT * cohesion(agents)
    separation_force = SEPARATION_WEIGHT * separation(agents)
    alignment_force = ALIGNMENT_WEIGHT * alignment(agents, velocities)
    goal_force = GOAL_ATTRACTION_WEIGHT * attraction_to_goal(agents, GOAL)
    obstacle_force = AVOID_OBSTACLE_WEIGHT * avoid_obstacles(agents, obstacles)
    
    velocities += cohesion_force + separation_force + alignment_force + goal_force + obstacle_force
    velocities = np.clip(velocities, -VELOCITY_LIMIT, VELOCITY_LIMIT)
    
    agents = update_positions(agents, velocities)
    
    for i, agent in enumerate(agents):
        if np.linalg.norm(agent - GOAL) < np.linalg.norm(personal_best_positions[i] - GOAL):
            personal_best_positions[i] = agent
    
    if iteration % 10 == 0:
        plot_simulation(agents, obstacles, GOAL, iteration)




