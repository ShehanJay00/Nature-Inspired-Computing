import numpy as np
import pandas as pd
import random
import matplotlib.pyplot
%matplotlib inline


from sklearn.datasets import load_breast_cancer

from sklearn.model_selection import train_test_split 

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score


#import the breast cancer dataset 
from sklearn.datasets import load_breast_cancer

cancer=load_breast_cancer ()

df = pd. DataFrame (cancer ['data'],columns=cancer['feature_names'])

label=cancer ["target"]


df


#splitting the model into training and testing set
X_train, X_test, y_train, y_test = train_test_split(df,
label, test_size=0.30, random_state=101)


#training a logistics regression model
logmodel = LogisticRegression(max_iter=10000)

logmodel. fit (X_train,y_train)

predictions = logmodel.predict(X_test)

print ("Accuracy = "+ str(accuracy_score(y_test, predictions)))






#population logic and fitness logic explanation

import numpy as np

n_feat = 30  # Potential gene count inside chromosome to be defined. This matches with Breast cancer dataset's 30 features.

# Use bool or np.bool_ to define the type of elements in the array
chromosome = np.ones(n_feat, dtype=bool)  # Start with all genes inside chromosomes set to ones.

print("Chromosome before randomness =\n", chromosome)


chromosome [: int (0.3*n_feat)]=False # make 30% as false inside the chromosome. This is done to introduce random diversity print ("Chromosome"s after randomness =\n" , chromosome)
print("Chromosome's after randomness =\n", chromosome)


np. random.shuffle(chromosome) #randomly mix the falses by shuffling 
print ("Chromosome's after random shuffling =\n", chromosome)


population = []
population. append (chromosome)# introduce chromosome sequence to population list as individuals 
print ("Population formed =\n" ,population)


# after random shuffling introduced by 3%, all 30 features will not be used for traing. Only features corresponding with
# true have been utilized for training. So, training features have been controlled by this mechanism. iloc is used to
# random retrival of feature / columns corrsponding with true slots
print ("selected features used for training =\n",X_train.iloc[:, chromosome ])


# Main function for the genetic algorithm
def generations(size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, y_train, y_test):
    best_chromo = []
    best_score = []
    population_nextgen = initialization_of_population(size, n_feat)

    for i in range(n_gen):
        # Evaluate fitness scores for the population
        scores = evaluate_fitness(population_nextgen, X_train, y_train, X_test, y_test)

        # Display the top 2 scores
        print(f"Generation {i+1}, Top 2 Scores: {scores[np.argsort(scores)][-2:]}")

        # Select parents based on their fitness scores
        pop_after_sel = selection(population_nextgen, scores, n_parents)

        # Generate new population through crossover
        pop_after_cross = crossover(pop_after_sel, size)

        # Apply mutation to the new population
        population_nextgen = mutation(pop_after_cross, mutation_rate)

        # Track the best chromosome and its score
        best_chromo.append(population_nextgen[np.argmax(scores)])
        best_score.append(np.max(scores))

    return best_chromo, best_score


def initilization_of_population(size, n_feat):
    population = []
    for i in range(size):
        # Create a chromosome with all features initially set to True
        chromosome = np.ones(n_feat, dtype=bool)
        # Set approximately 30% of the features to False
        chromosome[:int(0.3 * n_feat)] = False
        # Shuffle the chromosome to distribute False values randomly
        np.random.shuffle(chromosome)
        population.append(chromosome)
    
    return np.array(population)


chromo,score=generations(size=20,n_feat=30,n_parents=8, mutation_rate=0.10, n_gen=20,X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test)

logmodel. fit(X_train.iloc[:,chromo[-1]],y_train)
              
predictions = logmodel.predict(X_test.iloc[ :,chromo[-1]])

print("Accuracy score after genetic algorithm is= "+str(accuracy_score(y_test, predictions)))





# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import random

# Load the breast cancer dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the genetic algorithm functions
def initialize_population(size, n_feat):
    population = []
    for i in range(size):
        chromosome = np.ones(n_feat, dtype=bool)
        chromosome[:int(0.3 * n_feat)] = False
        np.random.shuffle(chromosome)
        population.append(chromosome)
    return population

def fitness_score(population, X_train, y_train, X_test, y_test):
    scores = []
    for chromosome in population:
        logmodel = LogisticRegression(max_iter=10000)
        logmodel.fit(X_train.iloc[:, chromosome], y_train)
        predictions = logmodel.predict(X_test.iloc[:, chromosome])
        scores.append(accuracy_score(y_test, predictions))
    scores, population = np.array(scores), np.array(population)
    inds = np.argsort(scores)[::-1]
    return list(scores[inds]), list(population[inds, :])

def selection(pop_after_fit, n_parents):
    population_nextgen = []
    for i in range(n_parents):
        population_nextgen.append(pop_after_fit[i])
    return population_nextgen

def crossover(pop_after_sel):
    population_nextgen = pop_after_sel[:]
    for i in range(len(pop_after_sel)):
        child = pop_after_sel[i].copy()
        crossover_point = np.random.randint(1, len(child)-1)
        child[crossover_point:] = pop_after_sel[(i+1)%len(pop_after_sel)][crossover_point:]
        population_nextgen.append(child)
    return population_nextgen

def mutation(pop_after_cross, mutation_rate):
    population_nextgen = []
    for i in range(len(pop_after_cross)):
        chromosome = pop_after_cross[i].copy()
        for j in range(len(chromosome)):
            if random.random() < mutation_rate:
                chromosome[j] = not chromosome[j]
        population_nextgen.append(chromosome)
    return population_nextgen

def generations(size, n_feat, n_parents, mutation_rate, n_gen, X_train, X_test, y_train, y_test):
    population_nextgen = initialize_population(size, n_feat)
    best_chromo = []
    best_score = []
    
    for i in range(n_gen):
        scores, pop_after_fit = fitness_score(population_nextgen, X_train, y_train, X_test, y_test)
        print("Generation", i+1, "Top 2 Scores:", scores[:2])
        pop_after_sel = selection(pop_after_fit, n_parents)
        pop_after_cross = crossover(pop_after_sel)
        population_nextgen = mutation(pop_after_cross, mutation_rate)
        best_chromo.append(pop_after_fit[0])
        best_score.append(scores[0])
    
    return best_chromo, best_score

# Run the genetic algorithm
chromo, score = generations(size=20, n_feat=30, n_parents=8, mutation_rate=0.10, n_gen=20, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)

# Train and evaluate the model with the selected features
logmodel = LogisticRegression(max_iter=10000)
logmodel.fit(X_train.iloc[:, chromo[-1]], y_train)
predictions = logmodel.predict(X_test.iloc[:, chromo[-1]])
print("Accuracy score after genetic algorithm is= " + str(accuracy_score(y_test, predictions)))



selection



